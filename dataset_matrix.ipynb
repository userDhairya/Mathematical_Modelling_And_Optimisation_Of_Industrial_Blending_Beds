{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyM60EPsbUNpO4E4XyzmD6Uk"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "hzoj7H8g8JIo"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Assume the classes are already correctly imported\n",
    "from bmh.benchmark.material_deposition import MaterialDeposition, Material, Deposition\n",
    "from bmh.simulation.bsl_blending_simulator import BslBlendingSimulator\n",
    "\n",
    "# Function for a random walk\n",
    "def random_walk(n, start=5, step_size=0.1):\n",
    "    walk = [start]\n",
    "    for _ in range(n - 1):\n",
    "        step = np.random.uniform(-step_size, step_size)\n",
    "        next_value = max(0, walk[-1] + step)\n",
    "        walk.append(next_value)\n",
    "    return np.array(walk)\n",
    "\n",
    "# Function to calculate weighted average and standard deviation\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values - average) ** 2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "# Function to generate dataset\n",
    "def generate_dataset():\n",
    "    BED_SIZE_X = 59\n",
    "    BED_SIZE_Z = 20\n",
    "\n",
    "    deposition_timestamps = np.linspace(0, 100, 20)\n",
    "    material_timestamps = np.linspace(0, 100, 50)\n",
    "\n",
    "    x_positions = np.random.uniform(BED_SIZE_Z * 0.5, BED_SIZE_X - BED_SIZE_Z * 0.5, 20)\n",
    "\n",
    "    quality_values = random_walk(50, start=5, step_size=1)\n",
    "    quality_values = gaussian_filter(quality_values, sigma=1)\n",
    "    volume_values = np.ones(50) * 50\n",
    "\n",
    "    material = Material.from_data(pd.DataFrame({\n",
    "        'timestamp': material_timestamps,\n",
    "        'volume': volume_values,\n",
    "        'quality': quality_values\n",
    "    }))\n",
    "\n",
    "    deposition = Deposition.from_data(\n",
    "        data=pd.DataFrame({\n",
    "            'timestamp': deposition_timestamps,\n",
    "            'x': x_positions,\n",
    "            'z': [0.5 * BED_SIZE_Z] * len(x_positions)\n",
    "        }),\n",
    "        bed_size_x=BED_SIZE_X,\n",
    "        bed_size_z=BED_SIZE_Z,\n",
    "        reclaim_x_per_s=6\n",
    "    )\n",
    "\n",
    "    material_deposition = MaterialDeposition(material=material, deposition=deposition)\n",
    "\n",
    "    sim = BslBlendingSimulator(bed_size_x=BED_SIZE_X, bed_size_z=BED_SIZE_Z)\n",
    "    reclaimed_material = sim.stack_reclaim(material_deposition)\n",
    "\n",
    "    reclaimed_quality = reclaimed_material.data['quality']\n",
    "    reclaimed_volume = reclaimed_material.data['volume']\n",
    "\n",
    "    standard_deviation = weighted_avg_and_std(reclaimed_quality, reclaimed_volume)[1]\n",
    "\n",
    "    output_data = pd.DataFrame({\n",
    "        'y1': [standard_deviation],\n",
    "        **{f'x{i+1}': [quality] for i, quality in enumerate(material.data['quality'])},\n",
    "        **{f'x{i+51}': [x] for i, x in enumerate(deposition.data['x'])}\n",
    "    })\n",
    "\n",
    "    return output_data\n",
    "\n",
    "# Generate 10,000 datasets and concatenate them\n",
    "output_folder = '/mnt/d/UoM/DATA72000_ERP/Code V2/blending-evaluation-master'\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "for _ in range(100000):\n",
    "    dataset = generate_dataset()\n",
    "    combined_data = pd.concat([combined_data, dataset], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "combined_file_path = f'{output_folder}/matrix_dataset_100,000.csv'\n",
    "combined_data.to_csv(combined_file_path, index=False)\n",
    "\n",
    "print(f\"Matrix dataset saved to {combined_file_path}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from typing import Optional, Dict\n",
    "\n",
    "# Assume the classes are already correctly imported\n",
    "from bmh.benchmark.material_deposition import MaterialDeposition, Material, Deposition\n",
    "from bmh.simulation.bsl_blending_simulator import BslBlendingSimulator\n",
    "from bmh.helpers.math import stdev, weighted_avg_and_std\n",
    "from bmh.helpers.stockpile_math import get_stockpile_height, get_stockpile_slice_volume\n",
    "from bmh.benchmark.material_deposition import Material\n",
    "\n",
    "# Function for a random walk\n",
    "def random_walk(n, start=5, step_size=0.1):\n",
    "    walk = [start]\n",
    "    for _ in range(n - 1):\n",
    "        step = np.random.uniform(-step_size, step_size)\n",
    "        next_value = max(0, walk[-1] + step)\n",
    "        walk.append(next_value)\n",
    "    return np.array(walk)\n",
    "\n",
    "# Function to calculate weighted average and standard deviation\n",
    "def weighted_avg_and_std(values, weights):\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values - average) ** 2, weights=weights)\n",
    "    return average, math.sqrt(variance)\n",
    "\n",
    "# ReclaimedMaterialEvaluator class\n",
    "class ReclaimedMaterialEvaluator:\n",
    "    def __init__(self, reclaimed: Material, x_min: Optional[float] = None, x_max: Optional[float] = None):\n",
    "        self.reclaimed = reclaimed\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "\n",
    "        # Caches\n",
    "        self._parameter_stdev: Optional[Dict[str, float]] = None\n",
    "        self._volume_stdev: Optional[float] = None\n",
    "\n",
    "    def get_volume_stdev(self) -> float:\n",
    "        if self._volume_stdev is None:\n",
    "            ideal_df = self.reclaimed.data.copy()\n",
    "            ideal_height = get_stockpile_height(volume=ideal_df['volume'].sum(), core_length=self.x_max - self.x_min)\n",
    "            ideal_df['x_diff'] = (ideal_df['x'] - ideal_df['x'].shift(1)).fillna(0.0)\n",
    "            ideal_df['volume'] = ideal_df.apply(\n",
    "                lambda row: get_stockpile_slice_volume(\n",
    "                    x=row['x'],\n",
    "                    core_length=self.x_max - self.x_min,\n",
    "                    height=ideal_height,\n",
    "                    x_min=self.x_min,\n",
    "                    x_diff=row['x_diff']\n",
    "                ), axis=1\n",
    "            )\n",
    "\n",
    "            self._volume_stdev = stdev((ideal_df['volume'] - self.reclaimed.data['volume']).values)\n",
    "\n",
    "        return self._volume_stdev\n",
    "\n",
    "# Function to generate dataset with F1 and F2\n",
    "def generate_dataset_with_f1_f2(file_id, output_folder):\n",
    "    BED_SIZE_X = 59\n",
    "    BED_SIZE_Z = 20\n",
    "\n",
    "    deposition_timestamps = np.linspace(0, 100, 20)\n",
    "    material_timestamps = np.linspace(0, 100, 50)\n",
    "\n",
    "    x_positions = np.random.uniform(BED_SIZE_Z * 0.5, BED_SIZE_X - BED_SIZE_Z * 0.5, 20)\n",
    "\n",
    "    quality_values = random_walk(50, start=5, step_size=1)\n",
    "    quality_values = gaussian_filter(quality_values, sigma=1)\n",
    "    volume_values = np.ones(50) * 50\n",
    "\n",
    "    material = Material.from_data(pd.DataFrame({\n",
    "        'timestamp': material_timestamps,\n",
    "        'volume': volume_values,\n",
    "        'quality': quality_values\n",
    "    }))\n",
    "\n",
    "    deposition = Deposition.from_data(\n",
    "        data=pd.DataFrame({\n",
    "            'timestamp': deposition_timestamps,\n",
    "            'x': x_positions,\n",
    "            'z': [0.5 * BED_SIZE_Z] * len(x_positions)\n",
    "        }),\n",
    "        bed_size_x=BED_SIZE_X,\n",
    "        bed_size_z=BED_SIZE_Z,\n",
    "        reclaim_x_per_s=6\n",
    "    )\n",
    "\n",
    "    material_deposition = MaterialDeposition(material=material, deposition=deposition)\n",
    "\n",
    "    sim = BslBlendingSimulator(bed_size_x=BED_SIZE_X, bed_size_z=BED_SIZE_Z)\n",
    "    reclaimed_material = sim.stack_reclaim(material_deposition)\n",
    "\n",
    "    reclaimed_quality = reclaimed_material.data['quality']\n",
    "    reclaimed_volume = reclaimed_material.data['volume']\n",
    "\n",
    "    standard_deviation_f1 = weighted_avg_and_std(reclaimed_quality, reclaimed_volume)[1]\n",
    "\n",
    "    evaluator = ReclaimedMaterialEvaluator(reclaimed=reclaimed_material, x_min=min(x_positions), x_max=max(x_positions))\n",
    "    standard_deviation_f2 = evaluator.get_volume_stdev()\n",
    "\n",
    "    output_data = pd.DataFrame({\n",
    "        'y1': [standard_deviation_f1],\n",
    "        'y2': [standard_deviation_f2],\n",
    "        **{f'x{i+1}': [quality] for i, quality in enumerate(material.data['quality'])},\n",
    "        **{f'x{i+51}': [x] for i, x in enumerate(deposition.data['x'])}\n",
    "    })\n",
    "\n",
    "    return output_data\n",
    "\n",
    "# Generate 100,000 datasets and concatenate them\n",
    "file_id = 1\n",
    "output_folder = '/mnt/d/UoM/DATA72000_ERP/Code V2/blending-evaluation-master'\n",
    "combined_data = pd.DataFrame()\n",
    "\n",
    "for _ in range(100000):\n",
    "    dataset = generate_dataset_with_f1_f2(file_id=file_id, output_folder=output_folder)\n",
    "    combined_data = pd.concat([combined_data, dataset], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset\n",
    "combined_file_path = f'{output_folder}/matrix_f1_f2_100,000.csv'\n",
    "combined_data.to_csv(combined_file_path, index=False)\n",
    "\n",
    "print(f\"Matrix dataset saved to {combined_file_path}\")\n"
   ],
   "metadata": {
    "id": "5aex0Tzrye1Y"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
